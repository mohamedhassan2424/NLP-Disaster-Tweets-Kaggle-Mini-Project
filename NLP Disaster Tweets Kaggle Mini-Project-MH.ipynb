{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'plotly'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-2d24c9a96d5a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgridspec\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mgridspec\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpatches\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mmpatches\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mplotly\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpress\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mseaborn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'plotly'"
     ]
    }
   ],
   "source": [
    "# Data manipulation and analysis\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Visualization libraries\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "import matplotlib.gridspec as gridspec\n",
    "import matplotlib.patches as mpatches\n",
    "%matplotlib inline\n",
    "\n",
    "# Text processing and visualization\n",
    "import re\n",
    "from wordcloud import WordCloud\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Deep learning framework\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Embedding, LSTM, GRU, Bidirectional\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Machine learning utilities\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Model training utilities\n",
    "from keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint\n",
    "\n",
    "# Additional utilities\n",
    "from collections import Counter\n",
    "import missingno as msno\n",
    "\n",
    "# Suppress warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] File train.csv does not exist: 'train.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-77c8fbed6553>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"train.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"test.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    674\u001b[0m         )\n\u001b[1;32m    675\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 676\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    678\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    446\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    447\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 448\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    449\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    450\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    878\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 880\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    881\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    882\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1112\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1113\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1114\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1115\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1116\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1889\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usecols\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1890\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1891\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1892\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1893\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] File train.csv does not exist: 'train.csv'"
     ]
    }
   ],
   "source": [
    "# Load the training dataset from CSV file\n",
    "training_data = pd.read_csv(\"training_set.csv\")\n",
    "\n",
    "# Import the test dataset from CSV file\n",
    "evaluation_set = pd.read_csv(\"evaluation_set.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display summary statistics for the evaluation dataset\n",
    "print('Evaluation Dataset Statistics:')\n",
    "display(evaluation_set.describe(include='all'))\n",
    "\n",
    "# Show the first few rows of the training dataset\n",
    "print('Training Dataset Preview:')\n",
    "display(training_data.head())\n",
    "\n",
    "# Present summary statistics for the training dataset\n",
    "print('Training Dataset Statistics:')\n",
    "display(training_data.describe(include='all'))\n",
    "\n",
    "# Exhibit the first few rows of the evaluation dataset\n",
    "print('Evaluation Dataset Preview:')\n",
    "display(evaluation_set.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values in the training dataset\n",
    "print('Null values in Train set')\n",
    "display(training_data.isna().sum())\n",
    "\n",
    "# Check for missing values in the evaluation dataset\n",
    "print('Null values in Train set')\n",
    "display(evaluation_set.isna().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the figure size\n",
    "plt.figure(figsize=(8,8))\n",
    "\n",
    "# Create a donut chart of the outcome variable distribution\n",
    "outcome_counts = training_data['label'].value_counts()\n",
    "colors = ['#ff9999','#66b3ff','#99ff99','#ffcc99']\n",
    "explode = (0.05, 0, 0, 0)  # To emphasize the first slice\n",
    "\n",
    "outcome_counts.plot(kind='pie', autopct='%1.1f%%', startangle=90, colors=colors, \n",
    "                    explode=explode, pctdistance=0.85, wedgeprops=dict(width=0.5))\n",
    "\n",
    "# Customize the plot\n",
    "plt.title('Distribution of Outcome Labels', fontsize=16)\n",
    "plt.ylabel('')  # Remove y-label\n",
    "plt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle\n",
    "\n",
    "# Add a circle at the center to create a donut chart\n",
    "center_circle = plt.Circle((0,0), 0.70, fc='white')\n",
    "fig = plt.gcf()\n",
    "fig.gca().add_artist(center_circle)\n",
    "\n",
    "# Display the plot\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_frequent_terms(dataframe, dataset_type, color_scheme, top_n=20):\n",
    "    # Set the plot style\n",
    "    sns.set_style('darkgrid')\n",
    "    \n",
    "    # Create the figure and axes\n",
    "    fig, ax = plt.subplots(figsize=(10, 10))\n",
    "\n",
    "    # Get the top N most frequent terms\n",
    "    frequent_terms = dataframe['term'].value_counts()[:top_n]\n",
    "    \n",
    "    # Create a horizontal bar plot\n",
    "    bars = sns.barplot(x=frequent_terms, y=frequent_terms.index, palette=color_scheme, ax=ax)\n",
    "    \n",
    "    # Add value labels to the bars\n",
    "    for container in bars.containers:\n",
    "        ax.bar_label(container, padding=5)\n",
    "\n",
    "    # Customize x-axis labels\n",
    "    ax.set_xticklabels([f'{int(i):,}' for i in ax.get_xticks()], fontsize=9)    \n",
    "    \n",
    "    # Customize y-axis labels\n",
    "    ax.set_yticklabels([t.get_text() for t in ax.get_yticklabels()], fontsize=11)\n",
    "    \n",
    "    # Set title and labels\n",
    "    plt.title(f'Top {top_n} Frequent Terms in {dataset_type} Dataset', fontsize=16, pad=20)\n",
    "    plt.xlabel('Frequency', fontsize=12)\n",
    "    plt.ylabel('Terms', fontsize=12)\n",
    "    \n",
    "    # Adjust layout and display the plot\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Visualize frequent terms for training and evaluation datasets\n",
    "visualize_frequent_terms(training_data, 'Training', 'viridis')\n",
    "visualize_frequent_terms(evaluation_set, 'Evaluation', 'magma')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_tweet_text(text):\n",
    "    # Convert text to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Initialize lemmatizer\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    # Remove stop words\n",
    "    text = \" \".join(word for word in word_tokenize(text) if word not in stopwords.words('english'))\n",
    "    \n",
    "    # Replace specific patterns\n",
    "    text = re.sub(r\"like\", \" \", text)\n",
    "    text = re.sub(r\"nt\", \" \", text)\n",
    "    \n",
    "    # Remove punctuation and special characters\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    \n",
    "    # Remove numbers\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    \n",
    "    # Remove HTML tags\n",
    "    text = re.sub(r\"<.*?>|&.*?;\", \" \", text)\n",
    "    \n",
    "    # Remove URLs\n",
    "    text = re.sub(r\"https?://\\S+|www\\.\\S+\", \" \", text)\n",
    "    \n",
    "    # Remove non-alphabetic characters\n",
    "    text = re.sub(r\"[^a-z]\", \" \", text)\n",
    "    \n",
    "    # Apply lemmatization\n",
    "    text = \" \".join(lemmatizer.lemmatize(word) for word in word_tokenize(text))\n",
    "    \n",
    "    # Remove single-character words\n",
    "    text = re.sub(r\"\\b\\w\\b\", \"\", text).strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Apply the cleaning function to the training and evaluation datasets\n",
    "training_data[\"processed_text\"] = training_data[\"text\"].apply(clean_tweet_text)\n",
    "evaluation_set[\"processed_text\"] = evaluation_set[\"text\"].apply(clean_tweet_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the first few rows of the training dataset\n",
    "print(\"Preview of Training Dataset:\")\n",
    "display(training_data.head(5))\n",
    "\n",
    "# Show summary statistics for the training dataset\n",
    "print(\"\\nSummary Statistics of Training Dataset:\")\n",
    "display(training_data.describe())\n",
    "\n",
    "# Display the first few rows of the evaluation dataset\n",
    "print(\"\\nPreview of Evaluation Dataset:\")\n",
    "display(evaluation_set.head(5))\n",
    "\n",
    "# Show summary statistics for the evaluation dataset\n",
    "print(\"\\nSummary Statistics of Evaluation Dataset:\")\n",
    "display(evaluation_set.describe())\n",
    "\n",
    "# Display information about the datasets\n",
    "print(\"\\nTraining Dataset Info:\")\n",
    "training_data.info()\n",
    "\n",
    "print(\"\\nEvaluation Dataset Info:\")\n",
    "evaluation_set.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize lists for different tweet categories\n",
    "non_emergency = []\n",
    "emergency = []\n",
    "\n",
    "# Process non-emergency tweets\n",
    "for tweet in train.loc[train['target'] == 0, 'processed_text'].dropna():\n",
    "    for word in tweet.lower().strip().split():\n",
    "        non_emergency.append(word)\n",
    "\n",
    "# Process emergency tweets\n",
    "for tweet in train.loc[train['target'] == 1, 'processed_text'].dropna():\n",
    "    for word in tweet.lower().strip().split():\n",
    "        emergency.append(word)\n",
    "\n",
    "# Set up the plot\n",
    "plt.figure(figsize=(20, 20))\n",
    "\n",
    "# Create and display non-emergency word cloud\n",
    "plt.subplot(1, 2, 1)\n",
    "normal_cloud = WordCloud(background_color=\"white\", max_font_size=400, width=500, height=500, stopwords=\"english\", random_state=42, repeat=True)\n",
    "normal_cloud.generate(' '.join(non_emergency))\n",
    "plt.title(\"Tweets Unrelated to Emergencies\", size=15)\n",
    "plt.imshow(normal_cloud)\n",
    "\n",
    "# Create and display emergency word cloud\n",
    "plt.subplot(1, 2, 2)\n",
    "crisis_cloud = WordCloud(background_color=\"white\", max_font_size=400, width=500, height=500, stopwords=\"english\", random_state=42, repeat=True)\n",
    "crisis_cloud.generate(' '.join(emergency))\n",
    "plt.title(\"Emergency-related Tweets\", size=15)\n",
    "plt.imshow(crisis_cloud)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of characters in each tweet\n",
    "train['word_count'] = train[\"text\"].apply(len)\n",
    "test['word_count'] = test[\"text\"].apply(len)\n",
    "\n",
    "# Display summary statistics for word counts in the training set\n",
    "train['word_count'].describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_frequency(text_series):\n",
    "    frequency = Counter()\n",
    "    for entry in text_series.values:\n",
    "        for word in entry.split():\n",
    "            frequency[word] += 1\n",
    "    return frequency\n",
    "\n",
    "# Calculate word frequencies in the preprocessed text\n",
    "word_counts = word_frequency(train.preprocessed_text)\n",
    "\n",
    "# Determine the number of unique words\n",
    "unique_word_count = len(word_counts)\n",
    "\n",
    "# Display the count of unique words\n",
    "unique_word_count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare features and target variable\n",
    "features = train.processed_text\n",
    "target = train.label\n",
    "\n",
    "# Divide dataset into training and validation sets\n",
    "features_train, features_val, target_train, target_val = train_test_split(\n",
    "    features, target, test_size=0.15, random_state=42\n",
    ")\n",
    "\n",
    "# Convert to numpy arrays for compatibility with some ML libraries\n",
    "features_train = np.array(features_train)\n",
    "features_val = np.array(features_val)\n",
    "target_train = np.array(target_train)\n",
    "target_val = np.array(target_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize tokenizer with vocabulary size\n",
    "text_tokenizer = Tokenizer(num_words=unique_word_count)\n",
    "\n",
    "# Fit tokenizer on training data\n",
    "text_tokenizer.fit_on_texts(features_train)\n",
    "\n",
    "# Transform text to numerical sequences\n",
    "train_seq = text_tokenizer.texts_to_sequences(features_train)\n",
    "val_seq = text_tokenizer.texts_to_sequences(features_val)\n",
    "\n",
    "# Define maximum sequence length\n",
    "seq_max_len = 20\n",
    "\n",
    "# Pad sequences to ensure uniform length\n",
    "train_padded_seq = pad_sequences(train_seq, maxlen=seq_max_len, padding=\"post\", truncating=\"post\")\n",
    "val_padded_seq = pad_sequences(val_seq, maxlen=seq_max_len, padding=\"post\", truncating=\"post\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for available GPUs and configure memory growth\n",
    "gpu_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "print(\"Number of GPUs detected: \", len(gpu_devices))\n",
    "\n",
    "# Enable dynamic memory allocation for the first GPU\n",
    "if gpu_devices:\n",
    "    tf.config.experimental.set_memory_growth(gpu_devices[0], True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the bidirectional LSTM model\n",
    "bi_lstm_model = keras.models.Sequential([\n",
    "    Embedding(input_dim=unique_word_count, output_dim=32, input_length=seq_max_len),\n",
    "    Bidirectional(LSTM(256, return_sequences=True, dropout=0.1)),\n",
    "    Bidirectional(LSTM(256, return_sequences=True, dropout=0.1)),\n",
    "    LSTM(256),\n",
    "    Dropout(0.3),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Display model architecture\n",
    "bi_lstm_model.summary()\n",
    "\n",
    "# Configure optimizer\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "adam_optimizer = Adam(learning_rate=0.0001)\n",
    "\n",
    "# Compile the model\n",
    "bi_lstm_model.compile(optimizer=adam_optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Set up early stopping\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "stop_early = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "# Train the model\n",
    "training_history = bi_lstm_model.fit(\n",
    "    train_padded_seq, target_train, \n",
    "    epochs=10, \n",
    "    batch_size=64, \n",
    "    validation_data=(val_padded_seq, target_val), \n",
    "    callbacks=[stop_early]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training and validation accuracy over epochs\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(training_history.history['accuracy'], label='Train Accuracy')\n",
    "plt.plot(training_history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title('Model Accuracy Over Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.legend(loc='upper left')\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "# Plot training and validation loss over epochs\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(training_history.history['loss'], label='Train Loss')\n",
    "plt.plot(training_history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Model Loss Over Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.legend(loc='upper left')\n",
    "plt.grid()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the CNN model architecture\n",
    "cnn_classifier = Sequential([\n",
    "    Embedding(input_dim=unique_word_count, output_dim=128, input_length=seq_max_len),\n",
    "    Conv1D(filters=64, kernel_size=3, activation='relu'),\n",
    "    MaxPooling1D(pool_size=2),\n",
    "    Conv1D(filters=32, kernel_size=3, activation='relu'),\n",
    "    MaxPooling1D(pool_size=2),\n",
    "    Flatten(),\n",
    "    Dense(10, activation='relu'),\n",
    "    Dropout(0.35),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Configure optimizer\n",
    "adam_opt = Adam(learning_rate=0.0001)\n",
    "\n",
    "# Compile the model\n",
    "cnn_classifier.compile(loss='binary_crossentropy', optimizer=adam_opt, metrics=['accuracy'])\n",
    "\n",
    "# Display model summary\n",
    "cnn_classifier.summary()\n",
    "\n",
    "# Set up early stopping mechanism\n",
    "halt_early = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "# Train the CNN model\n",
    "cnn_training_history = cnn_classifier.fit(\n",
    "    train_padded_seq, target_train,\n",
    "    epochs=20,\n",
    "    batch_size=32,\n",
    "    validation_data=(val_padded_seq, target_val),\n",
    "    callbacks=[halt_early]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot training and validation accuracy\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(cnn_training_history.history['accuracy'], label='Train Accuracy')\n",
    "plt.plot(cnn_training_history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title('CNN Model Accuracy Over Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.legend(loc='upper left')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Plot training and validation loss\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(cnn_training_history.history['loss'], label='Train Loss')\n",
    "plt.plot(cnn_training_history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('CNN Model Loss Over Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.legend(loc='upper left')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform test data into numerical sequences\n",
    "test_seq = text_tokenizer.texts_to_sequences(test.processed_text)\n",
    "\n",
    "# Ensure uniform sequence length through padding\n",
    "test_padded_seq = pad_sequences(test_seq, maxlen=seq_max_len, padding=\"post\", truncating=\"post\")\n",
    "\n",
    "# Generate predictions using the trained bidirectional LSTM model\n",
    "prediction_probabilities = bi_lstm_model.predict(test_padded_seq)\n",
    "\n",
    "# Convert probabilities to binary class labels\n",
    "predicted_classes = (prediction_probabilities > 0.5).astype(int)\n",
    "\n",
    "# Append predicted labels to the test dataset\n",
    "test['model_prediction'] = predicted_classes\n",
    "\n",
    "test.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create a submission DataFrame\n",
    "result_df = pd.DataFrame({\n",
    "    'id': test_data.id,\n",
    "    'target': test_data.model_prediction\n",
    "})\n",
    "\n",
    "# Save the results to a CSV file\n",
    "result_df.to_csv('bi_lstm_results.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform test data into numerical sequences using the same tokenizer\n",
    "test_sequences = text_tokenizer.texts_to_sequences(test.processed_text)\n",
    "\n",
    "# Pad the sequences to ensure uniform length\n",
    "test_padded_seq = pad_sequences(test_sequences, maxlen=seq_max_len, padding=\"post\", truncating=\"post\")\n",
    "\n",
    "# Generate predictions for the test data using the CNN model\n",
    "cnn_predictions = model_cnn.predict(test_padded_seq)\n",
    "\n",
    "# Convert predicted probabilities to binary class labels (0 or 1)\n",
    "predicted_classes = (cnn_predictions > 0.5).astype(int)\n",
    "\n",
    "# Append predicted labels to the test DataFrame\n",
    "test['model_prediction'] = predicted_classes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame for submission\n",
    "submission_df = pd.DataFrame({\n",
    "    'id': test.id,\n",
    "    'target': test.model_prediction  # Use the updated column name\n",
    "})\n",
    "\n",
    "# Save the submission DataFrame to a CSV file\n",
    "submission_df.to_csv('cnn_model_submission.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image(filename=\"/path/to/your/image/Kaggle_score_NLP.png\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
